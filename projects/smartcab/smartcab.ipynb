{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smartcab project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm summary\n",
    "\n",
    "### initialization\n",
    "- initialize  Q-value table with 0 for unknown combinations of states and actions\n",
    "- select random action from all available action states\n",
    "\n",
    "### choose best action according to Q-state table or random \n",
    "- set \\\\(\\epsilon\\\\) (epsilon) as the probability of randomly selected action\n",
    "- greedy strategy of action selection\n",
    "    - choose best action \\\\(\\mathop{\\arg\\max}_{a} Q(s,a)\\\\)  at the probability of 1-\\\\(\\epsilon\\\\)\n",
    "    - choose random action from all available action states\n",
    "    \n",
    "### act and update Q-state table (learning)\n",
    "- act, get reward \\\\(r\\\\) of selected action in current environment and new state \\\\(s'\\\\)\n",
    "- choose best action for new state according to the current Q-table \\\\(a' = \\mathop{\\arg\\max}_{a} Q(s',a)\\\\) \n",
    "- set \\\\(\\alpha\\\\) parameter for Q-learning rate, and \\\\(\\gamma\\\\) parameter for Q-learning discount rate\n",
    "- update Q-state by $$ Q(s,a) = (1-\\alpha) Q(s,a) + \\alpha (r + \\gamma Q(s', a'))  $$\n",
    "\n",
    "### loop action selection and Q-state update process until \n",
    "- Q-state becomes stable\n",
    "- other conditions of ending the program (e.g. reach the destination / hard timelimit excelled in this project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smartcab learning goals\n",
    "\n",
    "Provided with the environment and planner classes, we want to implement a Q-learning agent that can find a optimal way towards the destination without severe penalties of not following traffic rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## procedure\n",
    "\n",
    "### Implement a Basic Driving Agent\n",
    "\n",
    "To begin, your only task is to get the **smartcab** to move around in the enviro\n",
    "nment. At this point, you will not be concerned with any sort of optimal driving\n",
    " policy. Note that the driving agent is given the following information at each \n",
    "intersection:\n",
    "- The next waypoint location relative to its current location and heading.\n",
    "- The state of the traffic light at the intersection and the presence of oncomin\n",
    "g vehicles from other directions.\n",
    "- The current time left from the allotted deadline.\n",
    "\n",
    "To complete this task, simply have your driving agent choose a random action fro\n",
    "m the set of possible actions (`None`, `'forward'`, `'left'`, `'right'`) at each\n",
    " intersection, disregarding the input information above. Set the simulation dead\n",
    "line enforcement, `enforce_deadline` to `False` and observe how it performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from smartcab.environment import Agent, Environment\n",
    "from smartcab.planner import RoutePlanner\n",
    "from smartcab.simulator import Simulator\n",
    "from smartcab.agent import LearningAgent\n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 'forward', 'left', 'right']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Environment()\n",
    "a = e.create_agent(LearningAgent)\n",
    "a.env.valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.env.valid_actions[np.random.randint(3)]  # choose a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION:*** _Observe what you see with the agent's behavior as it takes random actions. Does the **smartcab** eventually make it to the destination? Are there any other interesting observations to note?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer**: With a random walk, in 100 trials, about half of the smartcab have reached the destination within the hard time limit(100), but not within the deadline. This observation may depend on our route planner which always point to the final destination, and simple grid network(8*6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inform the Driving Agent\n",
    "\n",
    "Now that your driving agent is capable of moving around in the environment, your next task is to identify a set of states that are appropriate for modeling the **smartcab** and environment. The main source of state variables are the current inputs at the intersection, but not all may require representation. You may choose to explicitly define states, or use some combination of inputs as an implicit state. At each time step, process the inputs and update the agent's current state using the `self.state` variable. Continue with the simulation deadline enforcement `enforce_deadline` being set to `False`, and observe how your driving agent now reports the change in state as the simulation progresses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'heading': (0, 1), 'location': (8, 3)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.env.agent_states[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define state as combination of inputs and next_waypoint planner\n",
    "self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "inputs = self.env.sense(self)\n",
    "self.state = (inputs['light'], inputs['oncoming'], inputs['left'], inputs['right'], self.next_waypoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION:*** _What states have you identified that are appropriate for modeling the **smartcab** and environment? Why do you believe each of these states to be appropriate for this problem?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer **: I have defined the states from combinations of environment variables at the intersection(light, oncoming traffic, left traffic, right traffic) and the direction of our cab provided by a simple planner. Because the current environment and direction both affects the reward(penalty), I think these are the proper components for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***OPTIONAL:*** _How many states in total exist for the **smartcab** in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer **: There are 2 (light) *  4 (actions for oncoming cars) * 4 (actions for left cars) * 4 (actions for right cars) * 4 (next_waypoint, actions for the cab to act according to simple plan) = 512 combinations. Given current goal of reaching the destinations in maximum( 100, 5 * distance) steps, it's hot to learn all states.  I ran 100 rounds of experiment in current problem setting, and get 64 states in total,  because our system have default 3 dummy cars on the 8*6 traffic grids, in mostly case there are no oncoming, left, right cars at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Q-Learning Driving Agent\n",
    "\n",
    "With your driving agent being capable of interpreting the input information and \n",
    "having a mapping of environmental states, your next task is to implement the Q-L\n",
    "earning algorithm for your driving agent to choose the *best* action at each tim\n",
    "e step, based on the Q-values for the current state and action. Each action take\n",
    "n by the **smartcab** will produce a reward which depends on the state of the en\n",
    "vironment. The Q-Learning driving agent will need to consider these rewards when\n",
    " updating the Q-values. Once implemented, set the simulation deadline enforcemen\n",
    "t `enforce_deadline` to `True`. Run the simulation and observe how the **smartca\n",
    "b** moves about the environment in each trial.\n",
    "\n",
    "The formulas for updating Q-values can be found in [this](https://classroom.udac\n",
    "ity.com/nanodegrees/nd009/parts/0091345409/modules/e64f9a65-fdb5-4e60-81a9-72813\n",
    "beebb7e/lessons/5446820041/concepts/6348990570923) video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def __init__(self, env):\n",
    "        super(LearningAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "        self.epsilon = 0.5   # probability of randomly select action\n",
    "        self.alpha = 0.3  # Q-value learning rate\n",
    "        self.discount = 0.9  # discount for future rewards\n",
    "        self.Qvalue = OrderedDict()  # initialize Q-value table\n",
    "        self.previous_state = None # initialize previous state  \n",
    "        self.rounds = 1  # initialize training rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "\n",
    "        previous_state = self.env.agent_states[self]\n",
    "        previous_location = previous_state['location']\n",
    "        previous_heading = previous_state['heading']\n",
    "\n",
    "        # TODO: Update state with sensor information and guided directions towards destination\n",
    "\n",
    "        self.previous_state = (inputs['light'], inputs['oncoming'], inputs['left'], inputs['right'], self.next_waypoint)\n",
    "\n",
    "        for a in self.env.valid_actions:  # initialize Q-value table for unknown combinations of states and actions\n",
    "            if (self.previous_state, a) not in self.Qvalue.iterkeys():\n",
    "                self.Qvalue[(self.previous_state, a)] = 0\n",
    "           \n",
    "        prob = np.array([self.Qvalue[(self.previous_state, None)] , self.Qvalue[(self.previous_state, 'forward')], self.Qvalue[(self.previous_state, 'left')], self.Qvalue[(self.previous_state, 'right')]])\n",
    "\n",
    "        # TODO: Select action according to your policy\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.env.valid_actions)\n",
    "        else:\n",
    "            action = self.env.valid_actions[np.random.choice(np.where(prob == prob.max())[0])]  \n",
    "            # prefer this rather than argmax, which can break the tie preference \n",
    " \n",
    "\n",
    "        # Execute action and get reward\n",
    "        oldValue = self.Qvalue[(self.previous_state, action)]\n",
    "        reward = self.env.act(self, action)\n",
    "        \n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "\n",
    "        state = self.env.agent_states[self]\n",
    "        location = state['location']\n",
    "        heading = state['heading']\n",
    "\n",
    "        nextInputs = self.env.sense(self)\n",
    "        #print inputs, nextInputs\n",
    "        self.state = (nextInputs['light'], nextInputs['oncoming'], nextInputs['left'], nextInputs['right'], self.planner.next_waypoint())\n",
    "\n",
    "        for a in self.env.valid_actions: # initialize Q-value table for unknown combinations of current states and actions\n",
    "            if (self.state, a) not in self.Qvalue.iterkeys():\n",
    "                self.Qvalue[(self.state, a)] = 0\n",
    "\n",
    "        futureValue = max([self.Qvalue[(self.state, a)] for a in self.env.valid_actions])\n",
    "        newValue = reward + self.discount * futureValue \n",
    "        self.Qvalue[(self.previous_state, action)] =  (1-self.alpha) * oldValue + self.alpha * newValue  \n",
    "        # update Q-value table balanced by learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION:*** _What changes do you notice in the agent's behavior when compare\n",
    "d to the basic driving agent when random actions were always taken? Why is this \n",
    "behavior occurring?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer **: according to my current setting of parameters, I can reach the destination in 45 rounds out of 100 trials within the deadline, that's a great learning process as we prefer the best action according to the up-to-date Q-table at 50% chance, which is a lot more reasonable than random choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the Q-Learning Driving Agent\n",
    "\n",
    "Your final task for this project is to enhance your driving agent so that, after\n",
    " sufficient training, the **smartcab** is able to reach the destination within t\n",
    "he allotted time safely and efficiently. Parameters in the Q-Learning algorithm,\n",
    " such as the learning rate (`alpha`), the discount factor (`gamma`) and the expl\n",
    "oration rate (`epsilon`) all contribute to the driving agent?s ability to learn \n",
    "the best action for each state. To improve on the success of your **smartcab**:\n",
    "- Set the number of trials, `n_trials`, in the simulation to 100.\n",
    "- Run the simulation with the deadline enforcement `enforce_deadline` set to `Tr\n",
    "ue` (you will need to reduce the update delay `update_delay` and set the `displa\n",
    "y` to `False`).\n",
    "- Observe the driving agent?s learning and **smartcab?s** success rate, particul\n",
    "arly during the later trials.\n",
    "- Adjust one or several of the above parameters and iterate this process.\n",
    "\n",
    "This task is complete once you have arrived at what you determine is the best co\n",
    "mbination of parameters required for your driving agent to learn successfully. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I decrease the epsilon as the training time increases, but not any further beyond a reasonable threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def reset(self, destination=None):\n",
    "        self.planner.route_to(destination)\n",
    "        # TODO: Prepare for a new trip; reset any variables here, if required\n",
    "        self.rounds += 1 # increase count of training rounds \n",
    "        if self.rounds > 100:\n",
    "            self.rounds = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def update(self, t):\n",
    "        self.epsilon = self.epsilon * (100-self.rounds)/100  # decrease epsilon with increasing training rounds as confidence gains along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION:*** _Report the different values for the parameters tuned in your ba\n",
    "sic implementation of Q-Learning. For which set of parameters does the agent per\n",
    "form best? How well does the final driving agent perform?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        self.epsilon = 0.9   # probability of randomly select action\n",
    "        self.alpha = 0.2  # Q-value learning rate\n",
    "        self.discount = 0.9  # discount for future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I finally choose the above parameters as the result shows the cab reach the destination in 99 rounds out of 100 trials most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION:*** _Does your agent get close to finding an optimal policy, i.e. re\n",
    "ach the destination in the minimum possible time, and not incur any penalties? H\n",
    "ow would you describe an optimal policy for this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After 100 rounds of training, some highest Q-values for each state are as follows, which correctly obey the traffic rules:\n",
    "- when there is green light and no surrounding cars, you can move in your direction\n",
    "(('green', None, None, None, 'right'), 'right') 7.64943183796\n",
    "(('green', None, None, None, 'forward'), 'forward') 9.21107577481\n",
    "(('green', None, None, None, 'left'), 'left') 5.90768926833\n",
    "- when there is red light and no surrounding cars, you can turn right if your direction is 'right'\n",
    "(('red', None, None, None, 'right'), 'right') 5.36415337755\n",
    "- when there is green light, and right car turns left, your direction is right, you can turn right\n",
    "(('green', None, None, 'left', 'right'), 'right') 3.18866840061\n",
    "- when there is green light, and right car turns right, your direction is forward, you can go forward\n",
    "(('green', None, None, 'right', 'forward'), 'forward') 3.84675308795\n",
    "\n",
    "Observed with the final 5 test rounds, except only one little penalty for a single step, all the actions to the destination follow the optimal route, without idle time lost. I think given the not thorough Q-table, this is reasonable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
